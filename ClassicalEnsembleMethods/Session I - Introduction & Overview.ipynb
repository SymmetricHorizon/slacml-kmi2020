{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview & Background\n",
    "\n",
    "**Machine Learning**: Developing Algorithms and models that improve their performance at a task by processing data.\n",
    "\n",
    "**Data**: Sets of features, $x_i$ and (optionally) labels $y_i$. A *label* is the ideal answer we'd like our ML model to give when inputed the features $x$.\n",
    "\n",
    "In **Supervised Learning**, our model expects to train on both features and labels. The task is to construct an model which is able to predict the label of an object given the set of features.\n",
    "\n",
    "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
    "In *classification* problems, the labels are discrete, and dont come from a metric space. An example of classification is to determine if a person has the flu based on their vital signs.\n",
    "In *regression* problems, the labels are continous and from a metric space. An example of regression is to determine a person's body temperature, given their vital signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercises in this section, we shall be using the [scikit-learn](http://scikit-learn.org) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features). For Supervised learning problems, data in scikit-learn is represented as a **feature matrix** and a **label vector**\n",
    "\n",
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here there are $N$ samples and $D$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning: Regression\n",
    "\n",
    "**Exercise 1** \n",
    "\n",
    "a) Please use numpy to create a data set with 50 points corresponding to a line with slope 2 and intercept 5, with standard normal noise at each point. Please arrange this data with all the x-values as a feature matrix, and the y values as the label vector. ($y_i=2 \\times x_i + 5 + \\epsilon_i$)\n",
    "\n",
    "b) Create a scatter plot of these using matplotlib.\n",
    "\n",
    "(I've started the exercise code for you. The solutions are at the bottom of this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.linspace(0,10,50)...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Normalize Data\n",
    "\n",
    "\n",
    "![alt text](Errata/Normalization.jpg)\n",
    "\n",
    "\n",
    "\n",
    "For gradient descent type applications, we'd like all features to have similar means, variances and ranges; so that the error surface is properly shaped. Else, we can get into wild oscillations during optimization iterations and even a loss of convergence.\n",
    "\n",
    "The MinMaxScaler() is one of the data scaling and normalization functions available. \n",
    "It takes a **2D** matrix as input to be scaled and (optionally) the range to be scaled to.\n",
    "\n",
    "It applies a 2 step transform:\n",
    "\n",
    "Step 1: X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "Step 2: X_scaled = X_std * (max_of_range - min_of_range) + min_of_range\n",
    "\n",
    "To use this scaler, \n",
    "\n",
    "a) Instantiate the scaler with the range that you'd like to scale to,\n",
    "\n",
    "b) fit the scaler on your data,\n",
    "\n",
    "c) Use this fitted scaler to transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX=MinMaxScaler(feature_range=(-1,1)) #instantiate with the range\n",
    "scalerX.fit(X)                             #Fit on data\n",
    "Xnorm=scalerX.transform(X)                 #Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerY=MinMaxScaler(feature_range=(-1,1))\n",
    "scalerY.fit(y[:,np.newaxis]) \n",
    "ynorm=scalerY.transform(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xnorm,ynorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "When you're making your final plots, you may want the answers to be unscaled.\n",
    "In that case, use the inverse_transform() method of the trained scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xunscaled=scalerX.inverse_transform(Xnorm)\n",
    "yunscaled=scalerY.inverse_transform(ynorm)\n",
    "plt.scatter(Xunscaled,yunscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Regression Model\n",
    "\n",
    "A basic scikit-learn supervised learning model, there are 3 steps:\n",
    "\n",
    "a) Instantiate: Declare an instance of a specific model,\n",
    "\n",
    "b) Train: Fit (or train) this model instance on data,\n",
    "\n",
    "c) Analyze: Use this trained model to make predictions for new data, analyze model coefficients.\n",
    "\n",
    "Let's see these steps in action, using a linear regression model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Declare an instance of a linear regressor\n",
    "model = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fit the model on data\n",
    "model.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions\n",
    "y_pred=model.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X,y,label=\"Data\")\n",
    "plt.plot(X,y_pred,label=\"Model Predictions\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Coefficients of Trained Model:\n",
    "print(\"Slope: \",model.coef_)\n",
    "print(\"Intercept: \",model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Error of trained Model:\n",
    "print (\"mean squared error:\", metrics.mean_squared_error(model.predict(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Curve-fitting and Machine Learning\n",
    "\n",
    "**Generalizability**: How well does my trained model perform on data that it has never seen? \n",
    "Exam questions analogy.\n",
    "\n",
    "**Dividing Learning Data set into train-test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** \n",
    "\n",
    "a) Use this split dataset to train and evaluate a (new) Linear Regression model. Use the train dataset (X_train, y_train) to train the model. Use the test dataset to evaluate the performance. \n",
    "\n",
    "b) Plot the model predictions on the test dataset versus the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression: Using higher order fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_regression_data(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,1,100)[:,np.newaxis]\n",
    "y=complex_regression_data(X)\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "X_test = np.linspace(-0.01, 1.01, 500)[:, None]\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "print (\"mean squared error:\", metrics.mean_squared_error(model.predict(X), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression(LinearRegression):\n",
    "    def __init__(self, degree=1, **kwargs):\n",
    "        self.degree = degree\n",
    "        LinearRegression.__init__(self, **kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if X.shape[1] != 1:\n",
    "            raise ValueError(\"Only 1D data, Please!\")\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return LinearRegression.fit(self, Xp, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return LinearRegression.predict(self, Xp)\n",
    "    \n",
    "    def err(self,X,y):\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        yp=LinearRegression.predict(self, Xp)\n",
    "        return np.mean((y-yp)*(y-yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(degree=42)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "print (\"mean squared error:\", metrics.mean_squared_error(model.predict(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & Validation Data\n",
    "\n",
    "*Parameters*: Variables describing the model that are tuned using the training data, for instance, the slope and the intercept, the weights & biases of a neural network.\n",
    "\n",
    "*Hyperparameters*: Variables describing the model that are not tuned using the training data, for instance, the degree of the model expression, the architecture of the neural network.\n",
    "\n",
    "Hyperparameters are traditionally set using validation or cross validation. In good old fashioned validation, you divide your original dataset into **3** parts: Training, validation (or \"holdout\") and testing.\n",
    "\n",
    "The training data is used to *tune* model parameters, \n",
    "\n",
    "the validation data is used to *select* model hyperparameters,\n",
    "\n",
    "the testing data is used to *report* an estimate for generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.6)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5)\n",
    "\n",
    "X_train.shape,X_val.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** \n",
    "\n",
    "Use this split dataset to train and evaluate a (new) Regression model. Select the best degree of the model by using the error on the validation dataset.\n",
    "The splits have been made for you.\n",
    "\n",
    "Create a loop over the degrees.\n",
    "\n",
    "For each iteration of the loop, instantiate a PolynomialRegression model with the correct degree.\n",
    "\n",
    "Train this instance on the training data.\n",
    "\n",
    "Evaluate the trained model on the validation dataset.\n",
    "\n",
    "Plot the validation error versus the degree of the PolynomialRegression after the loop.\n",
    "\n",
    "Note: Let's only have degrees from 1 to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=range(1,9)\n",
    "e=np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    model = PolynomialRegression(degree=degrees[i])\n",
    "    .....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting & Regularization\n",
    "\n",
    "*Regularization*: Adding external information to a machine learning model to make it \"well-posed\".\n",
    "\n",
    "Traditionally, we express a preference towards simpler, smoother models by adding a penalty.\n",
    "\n",
    "Model Error: $\\sum (y_i-\\hat{y_i})^2 + \\lambda \\sum ||W_j||^2$\n",
    "\n",
    "L2 Regularization, aka, Ridge Regression.\n",
    "\n",
    "$\\lambda$, is the regularization parameter and is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Diabetes Dataset**: 442 measurements from diabetes patients on features like age, sex, BMI, Blood Pressure, Serum measurements. The target is quantitative measurement of the progress of diabetes one year after these measurements were taken. \n",
    "\n",
    "A **good** ML model would be useful not just to predict the future incidence for new patients, but also evaluate the importance of different features on the progress of the disease.\n",
    "\n",
    "Cite[Efron, Hastie, etc \"LARS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.data\n",
    "y=dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,2],y)\n",
    "plt.xlabel(\"BMI (Normalized)\")\n",
    "plt.ylabel(\"Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=np.array([1,0.1,0.01,0.001,0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=GridSearchCV(estimator=model, param_grid=dict(alpha=alphas), cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning: Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Split the classification dataset into a training dataset with 80% of the samples and a testing dataset with 20% of the samples. Check and print the shapes of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, penalty='l2')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "plt.title('Test Data')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred)\n",
    "plt.title('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0,10,50)[:,np.newaxis]\n",
    "y = 2 * X.squeeze() + 5 + np.random.normal(size=50)\n",
    "\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "model2= LinearRegression(normalize=True)\n",
    "model2.fit(X_train,y_train)\n",
    "\n",
    "print (\"mean squared error:\", metrics.mean_squared_error(model.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "plt.figure()\n",
    "plt.scatter(X_test,y_test,label=\"Data\")\n",
    "plt.plot(X_test,y_pred,label=\"Model Predictions\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "degrees=range(1,16)\n",
    "e=np.zeros(15)\n",
    "\n",
    "for i in range(15):\n",
    "    model = PolynomialRegression(degree=degrees[i])\n",
    "    model.fit(X_train, y_train)\n",
    "    e[i]=model.err(X_val,y_val)\n",
    "    print(\"Degree: \", degrees[i],\" Error: \",e[i])\n",
    "    \n",
    "plt.plot(degrees,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
