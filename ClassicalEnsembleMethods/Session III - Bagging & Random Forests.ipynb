{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting turns out to be a property of decision trees. It is very easy to go too deep in the tree, and thus to fit details of the particular data sample, rather than the overall properties of the distributions they are drawn from. \n",
    "![alt text](Errata/Fig7.pdf \"I Spy With My Little Eye\")\n",
    "\n",
    "\n",
    "# Section 2. Random Forests\n",
    "In this scenario, we use Ensemble Methods and use a randomized set of such \"trees\" to create a \"forest\". The basis of ensemble methods lies in the famous Condorcet’s Jury Theorem: A group wants to arrive at the “correct” decision via majority vote, wherein each individual has a probability p of voting for the correct decision. What should the size of the group be for optimal performance?\n",
    "It has been shown that if the individual is doing better than random guessing, then the more independent adjudicators in the group, the better the chances of getting the \"right\" answer.\n",
    "\n",
    "Intuitive example: The performance of the “Ask The Audience” lifeline in Who Wants To Be A Millionaire? \n",
    "\n",
    "(92% accuracy for “Ask The Audience” vs only 65% for “Phone A Friend”)\n",
    "\n",
    "In this manner, Aggregating randomized models decreases the variance of the ensemble, while still retaining the low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Random Forests: Toy Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4,random_state=0, cluster_std=0.65)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='gist_rainbow')\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='gist_rainbow',clim=(y.min(), y.max()), zorder=3)\n",
    "xx, yy = np.meshgrid(np.linspace(np.min(X[:,0]),np.max(X[:,0]), num=200),np.linspace(np.min(X[:,1]),np.max(X[:,1]), num=200))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "n_classes = len(np.unique(y))\n",
    "contours = plt.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='gist_rainbow', zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "In this exercise, we will be applying random forest classifier model on a cancer dataset. The set consists of measurements of 569 patients, with features describing the tumors, and the target classifying the tumors as malignant or benign, thus it is a classification problem.\n",
    "\n",
    "We import this dataset, split it into training and testing sets, and train a random forest classifier on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Split the data into train and test sets (80%-20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Instantiate a random forest classifier model with 100 estimators and a random state of 42. \n",
    "# If you complete this exercise early, Feel free to test the max_depth=4/5/6... hyperparameter too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fit the model on the training data, \n",
    "#and evaluate its score on the training and the test data (model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving Insight from the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    print(data.feature_names[i],clf.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_features = [x for i,x in enumerate(data.feature_names)]\n",
    " \n",
    "def breast_cancer_feature_importances_plot(model):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    n_features = 30\n",
    "    plt.barh(range(n_features), clf.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), breast_cancer_features)\n",
    "    plt.title('Random Forest Classifier: Feature Importances')\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "    plt.ylim(-1, n_features)\n",
    " \n",
    "breast_cancer_feature_importances_plot(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter selection for the Random Forest, on the Iris Dataset\n",
    "\n",
    "A dataset with 150 samples from 3 different types of Iris flowers. The features are the length and width of the petals and sepals of each flower. The target is the type of Iris flower (setosa, virginica, versicolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators':[10,20], 'max_features':['sqrt',0.25]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier()\n",
    "clf=GridSearchCV(model,params,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin=clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fin.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's do this Iris classification problem, **properly**.\n",
    "First, divide the dataset into train and test samples (80%-20%).\n",
    "Now, use your training data to select the \"best\" hyperparameters for \n",
    "\n",
    "a. the number of estimators (from 10-500) \n",
    "\n",
    "b. the maximum features to be between 'sqrt' and different ratios of the toal number of features, from 0.1 to 1.0.\n",
    "\n",
    "c. the maximum depth of the trees in the ensemble, from 2 to 10.\n",
    "\n",
    "Train this selected model on the training data, and evaluate its performance on the test dataset.\n",
    "Report the accuracy of the final model on the training dataset and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 Random Forests: Time series Regression Example\n",
    "In the previous section we considered random forests within the context of classification. Random forests can also be made to work in the case of regression (that is, continuous rather than categorical variables). The estimator to use for this is the RandomForestRegressor, and the syntax is very similar to what we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(200)\n",
    "\n",
    "def modelts(x, sigma=0.3):\n",
    "    fast_oscillation = np.sin(5 * x)\n",
    "    slow_oscillation = np.sin(0.5 * x)\n",
    "    noise = sigma * rng.randn(len(x))\n",
    "    return slow_oscillation + fast_oscillation + noise\n",
    "\n",
    "y = modelts(x)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators=200)\n",
    "forest.fit(x[:, None], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = forest.predict(xfit[:, None])\n",
    "ytrue = modelts(xfit, sigma=0)\n",
    "\n",
    "plt.scatter(x, y,alpha=0.5)\n",
    "plt.plot(xfit, yfit, '-r');\n",
    "plt.plot(xfit, ytrue, '-k', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true model is shown in the smooth gray curve, while the random forest model is shown by the jagged red curve. As you can see, the non-parametric random forest model is flexible enough to fit the multi-period data, without us needing to specifying a multi-period model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The Diabetes Dataset: 442 measurements from diabetes patients on features like age, sex, BMI, Blood Pressure, Serum measurements. The target is quantitative measurement of the progress of diabetes one year after these measurements were taken. \n",
    "\n",
    "A good ML model would be useful not just to predict the future incidence for new patients, but also evaluate the importance of different features on the progress of the disease.\n",
    "\n",
    "We'll fit a Random Forest Regressor to this dataset and evaluate it's performance.\n",
    "\n",
    "a) Split the data into a train-test set (80%-20%)\n",
    "\n",
    "b) Use cross validation to select good hyperparameters for your Random Forest Regressor\n",
    "\n",
    "c) Train this selected Random forest model on your training data.\n",
    "\n",
    "d) evaluate it's performance on the test data.\n",
    "\n",
    "e) Report the relative importance of different features affecting the progress of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_diabetes()\n",
    "X=dataset.data\n",
    "y=dataset.target\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Random Forests: MNIST Dataset Classification\n",
    "As a final example, we apply the Random Forest Algorithm to the (too) well known MNIST dataset, for classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "fig = plt.figure(figsize=(3, 3)) \n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    ax.text(0, 4, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on training data: {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators':[50,100,200,500], 'max_features':['sqrt',0.1,0.3,0.5,0.7,1.0], 'max_depth':[2,5,7,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier()\n",
    "clf=GridSearchCV(model,params,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin=clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fin.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8 ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators':[50,100,200], 'max_features':['sqrt',0.1,0.3,0.5], 'max_depth':[5,7,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestRegressor()\n",
    "reg=GridSearchCV(model,params,cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin=reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fin.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestRegressor(n_estimators=50, max_features='sqrt',max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test), model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(dataset.feature_names[i],model.feature_importances_[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
