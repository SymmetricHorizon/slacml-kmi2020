{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model + Logistic regression + PyTorch\n",
    "\n",
    "In this notebook, we will introduce ourself to PyTorch machine learning framework by re-implement our linear regression model using PyTorch. Then we expand into a model with multiple linear transformation in the form of a simplest neural network. We exercise using a non-linear activation function and how that can impact the solution found by a model. \n",
    "\n",
    "### What we cover\n",
    "1. Logistic regression with a linear model using PyTorch\n",
    "2. Choice of an optimizer\n",
    "3. Irregular dataset that's not linearly separable\n",
    "4. Logistic regression on an irregular dataset\n",
    "5. Logistic regression with two layers perceptron\n",
    "6. ... + non-linear activation function and what it does\n",
    "7. Optimizers in Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = [8, 6]\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Revisiting a logistic regression with PyTorch\n",
    "\n",
    "We start with PyTorch library by revisiting a problem we already solved!\n",
    "\n",
    "Our challenge is to separate the red and blue dots using data instances with 2 input features.\n",
    "\n",
    "### Data sample generation\n",
    "\n",
    "Let's generate the dataset, the same one we did in the prevous hands-on would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_red_and_blue_normal(seed=123):\n",
    "    np.random.seed(seed)\n",
    "    sample_stat = 2000\n",
    "    # generate data set \"a\" as a combination of gaussian distributed position + label == 0\n",
    "    a = np.random.normal(1,1,sample_stat).reshape(int(sample_stat/2),2)\n",
    "    a = np.column_stack([a,np.zeros(shape=(len(a)),dtype=np.float32)])\n",
    "    # generate data set \"b\" as a combination of gaussian distributed position + label == 1\n",
    "    b  = np.random.normal(-1,1,sample_stat).reshape(int(sample_stat/2),2)\n",
    "    b = np.column_stack([b,np.ones(shape=(len(b)),dtype=np.float32)])\n",
    "    # Now combine both datasets and shuffle the ordering so it's random!\n",
    "    data = np.concatenate([a,b])\n",
    "    np.random.shuffle(data)\n",
    "    # Split them into input features (s) and label (y)\n",
    "    x,y = data[:,:2],data[:,2]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and always visualize to check it's not crazy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_red_and_blue_normal()\n",
    "\n",
    "def plot_data(x,y):\n",
    "    fig,ax=plt.subplots(figsize=(12,8),facecolor='w')\n",
    "    pos = y>0\n",
    "    neg = y<1\n",
    "    ax.plot(x[pos][:,0],x[pos][:,1],marker='o',markersize=6,linestyle='',color='red')\n",
    "    ax.plot(x[neg][:,0],x[neg][:,1],marker='x',markersize=6,linestyle='',color='blue')\n",
    "    plt.ylabel('$x_1$ ',fontsize=18)\n",
    "    plt.xlabel('$x_0$ ',fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch\n",
    "\n",
    "Let's first create PyTorch tensors (= change the datatype!)\n",
    "Remember we can just give a numpy data into `torch.Tensor` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_input = torch.Tensor(x).reshape(-1,2) # make sure the shape is (N,2) ... it actually is already so!\n",
    "torch_label = torch.Tensor(y).reshape(-1,1) # make sure the shape is (N,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our model in PyTorch. Check points are:\n",
    "\n",
    "1. Our model should inherit from `torch.nn.Module`\n",
    "2. Use `torch.nn.Linear` layer. Note bias is already handled so no \"bias trick\" needed!\n",
    "\n",
    "As mentioned in the linear regression example, actually, you can avoid defining your model as a class an use `torch.nn.Linear` and `torch.sigmoid` instances. But here we stick with defining a class since that would be how you will write your algorithms for any reasonably non-trivial research problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class model0(torch.nn.Module):\n",
    "    '''\n",
    "    Logistic regression model.\n",
    "    We name it model0 as we will make multiple similar models :)\n",
    "    '''\n",
    "    def __init__(self, num_features):\n",
    "        super(model0,self).__init__()\n",
    "        # Construct a linear operator for the specified number of input features.\n",
    "        # The output is only 1 feature. We want a constnat term = bias.\n",
    "        self._linear = torch.nn.Linear(in_features=num_features, out_features=1, bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        This should return our prediction (sigmoid output) for a binary classification\n",
    "        '''\n",
    "        return torch.sigmoid(self._linear(x))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a training loop. Remmeber 3 importnat steps:\n",
    "1. forward (compute loss)\n",
    "2. initialize gradients\n",
    "3. backward (apply gradients)\n",
    "\n",
    "... and also we introduce\n",
    "* `torch.nn.BCELoss` ... a binary cross-entropy (BCE) loss\n",
    "* `torch.optim.SGD` ... SGD optimizer (also used in the previous example of linear regression w/ PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(torch_input, torch_label, model, num_iterations=100, lr=0.001, optimizer='SGD'):\n",
    "    # Create a Binary-Cross-Entropy (BCE) loss module\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    # Create an optimizer\n",
    "    optimizer = getattr(torch.optim,optimizer)(model.parameters(),lr=lr)\n",
    "\n",
    "    # Now we run the training!\n",
    "    loss_v=[]\n",
    "    for idx in range(num_iterations): \n",
    "\n",
    "        # This is \"forward\" computation\n",
    "        prediction = model(torch_input) \n",
    "\n",
    "        # Compute the loss, reset the gradients, compute & apply gradients\n",
    "        loss = criterion(prediction, torch_label) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "\n",
    "        # Now let optimizer to \"take a step\"\n",
    "        optimizer.step() \n",
    "\n",
    "        # Record the loss\n",
    "        loss_v.append(loss.item())\n",
    "        \n",
    "    return loss_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model\n",
    "* Create our model with 2 input features\n",
    "* Call `train_torch` function with data, label, and our model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model \n",
    "model = model0(2) \n",
    "\n",
    "# Train\n",
    "loss = train_torch(torch_input,torch_label,model,num_iterations=50000)\n",
    "\n",
    "def plot_loss(loss_array):\n",
    "    # Plot the loss\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    fig,ax = plt.subplots(figsize=(12,8),facecolor='w')\n",
    "\n",
    "    if len(np.shape(loss_array))==1:\n",
    "        loss_array = [loss_array]\n",
    "    for idx,loss in enumerate(loss_array):\n",
    "        plt.plot(loss,marker='o',markersize=10,label='loss %d' % idx)\n",
    "    plt.ylabel('Loss',fontsize=18)\n",
    "    plt.xlabel('Iterations',fontsize=18)\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "\n",
    "So the loss looks like it learned something! But how do we retrieve the trained parameters?\n",
    "Yay Python, we can just directly access within the trained model ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dumping the weights...')\n",
    "print(model._linear.weight)\n",
    "print('\\nDumping the bias term...')\n",
    "print(model._linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the decision boundary line, $ax_0+bx_1+c$, like we did before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the torch tensor into a numpy array. \n",
    "# First, we have to \"detach\" any tensor for which gradient is automatically calculated.\n",
    "a,b = model._linear.weight.detach().numpy().reshape(-1)\n",
    "# Second, for a bias, we can just extract a single value by calling item() function.\n",
    "c = model._linear.bias.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then overlay this linear equation on the scatter plot of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_lines(x,y,lines):\n",
    "\n",
    "    fig,ax=plt.subplots(figsize=(12,8),facecolor='w')\n",
    "\n",
    "    # Draw data points\n",
    "    pos = y>0\n",
    "    neg = y<1\n",
    "    ax.plot(x[pos][:,0],x[pos][:,1],marker='o',markersize=6,linestyle='',color='red')\n",
    "    ax.plot(x[neg][:,0],x[neg][:,1],marker='x',markersize=6,linestyle='',color='blue')\n",
    "\n",
    "    # data range\n",
    "    x0min,x0max=x[:,0].min(),x[:,0].max()\n",
    "    x1min,x1max=x[:,1].min(),x[:,1].max()\n",
    "    \n",
    "    # Draw lines\n",
    "    if len(np.shape(lines)) == 1:\n",
    "        lines = [lines]\n",
    "    for a,b,c in lines:\n",
    "        x0vals = (x0min,x0max)\n",
    "        x1vals = (-a/b*x0min-c/b, -a/b*x0max-c/b)\n",
    "        ax.plot(x0vals,x1vals,linestyle='--',linewidth=4,marker='')\n",
    "\n",
    "    plt.ylabel('$x_1$ ',fontsize=18)\n",
    "    plt.xlabel('$x_0$ ',fontsize=18)\n",
    "    plt.ylim(x1min-2,x1max+2)\n",
    "    plt.xlim(x0min-2,x0max+2)\n",
    "    plt.show()\n",
    "\n",
    "plot_data_with_lines(x,y,[a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Voila!_ This looks reasonable to me :)\n",
    "\n",
    "## 2. Choice of an optimizer (Exercise 1)\n",
    "\n",
    "We mentioned that there are more than one falvor of gradient descent optimizer.\n",
    "\n",
    "Let's give a try to \"Adam\" optimizer.\n",
    "\n",
    "1. Re-train your model with `optimizer=\"Adam\"` argument to `train_torch` function and **store the new loss in a differently named attribute** (i.e. don't call it `loss` and overwrite the history from SGD training!).\n",
    "2. Plot both loss curves together (hint: `plot_loss` function takes an array of array as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see `Adam` can flatten the loss curve faster than SGD reaching about the same level of the loss value!\n",
    "\n",
    "Remember a difference depends on data, task, and your model to be optimized. There is no guaranteed \"best solution\". However, in practice, \"Adam\" is a very popular \"default\" choice for neural networks (later).\n",
    "\n",
    "## 2. Irregular (not linearly separable) dataset\n",
    "\n",
    "Let's now make our problem a bit more challenging by modifying our dataset. \n",
    "\n",
    "A new data generator puts red and blue points along straight lines of the same slope but with different offsets.\n",
    "\n",
    "A separation is hence straightforward, except we then re-distribute a half of red points somewhere else.\n",
    "\n",
    "It's probably the simplest to see by eyes. Let's generate the sample and visualize!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_red_and_blue_irregular(a,b,seed=123,distort=False):\n",
    "    np.random.seed(seed)\n",
    "    num_unit=100\n",
    "    x0 = np.zeros(shape=(num_unit*2),dtype=np.float32)\n",
    "    x0[:num_unit] = np.arange(0,3,3.0/num_unit)\n",
    "    x0[num_unit:] = np.arange(2,5,3.0/(num_unit))\n",
    "    x1 = a*x0 + b + np.random.normal(scale=0.3,size=num_unit*2)\n",
    "    x1[:num_unit] += 3\n",
    "    x1[num_unit:] -= 3\n",
    "\n",
    "    y = np.zeros(shape=(num_unit*2),dtype=np.int32)\n",
    "    y[:num_unit] = 0\n",
    "    y[num_unit:] = 1\n",
    "    \n",
    "    # if distort, shift some of label 0\n",
    "    if distort:\n",
    "        idx=np.arange(num_unit)\n",
    "        np.random.shuffle(idx)\n",
    "        idx=idx[:num_unit//2]\n",
    "        x0[idx] = np.random.normal(size=len(idx),scale=0.3)+2.5\n",
    "        x1[idx] = np.random.normal(size=len(idx),scale=0.3)\n",
    "    \n",
    "    \n",
    "    data = np.column_stack([x0,x1,y])\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data[:,:2],data[:,2]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same data generation function we used before\n",
    "a,b=2.0,4.0\n",
    "x,y=generate_red_and_blue_irregular(a,b,distort=True)\n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just look of it, it is clear we cannot separate two populations cleanly with a straight line. It is intuitively a difficult problem: I imagine different line slopes and offset but not very obvious what works the best immediately. A solution needs to wait till the later part of this notebook :)\n",
    "\n",
    "### Exercise 2\n",
    "Let's do just another practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=2.0,4.0\n",
    "x,y=generate_red_and_blue_irregular(a,b,distort=False)\n",
    "\n",
    "torch_input = torch.Tensor(x).reshape(-1,2)\n",
    "torch_label = torch.Tensor(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just generated another dataset with `distort=False`. \n",
    "1. Visualize this dataset\n",
    "2. Optimize `model0` for with `Adam` with 10000 steps.\n",
    "3. Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression on an irregular dataset (Exercise 3)\n",
    "\n",
    "Let's get back to a _distorted_ dataset and attempt logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=2.0,4.0\n",
    "x,y=generate_red_and_blue_irregular(a,b,distort=True)\n",
    "\n",
    "torch_input = torch.Tensor(x).reshape(-1,2)\n",
    "torch_label = torch.Tensor(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Try the same steps you have done for Exercise 2 with this distorted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, not so great :( \n",
    "\n",
    "## 5. Logistic regression with two linear layers\n",
    "\n",
    "During the lecture, we learned about adding a layer may help to linearize a non-linear dataset.\n",
    "\n",
    "Let's build a new model that contains 2 linear layers. This means we have two instances of `torch.nn.Linear`. Make the second instance, which is our linear classifier, to take 2 inputs so we can visualize as a line in 2D space. That means the first instance of `torch.nn.Linear` should produce 2 output features (imagine the first instance contain 2 linear transformations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model1(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,num_features):\n",
    "        super(model1,self).__init__()\n",
    "        # Define a linear model with a bias term\n",
    "        self._linear0  = torch.nn.Linear(num_features,2,bias=True)\n",
    "        self._linear1  = torch.nn.Linear(2,1,bias=True)\n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(self._linear1(self._linear0(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Train this new `model1` with the same dataset that is already generated with 1 additional task:\n",
    "* Visualize a linear decision boundary like we did before, but plot with data points after the first linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the result did not improve much compared to a single linear transformation (i.e. `model0`). That's because it's hard to distort the location of these data points, which is critical for whether a linear model works well or not, too much with only a linear transformation. \n",
    "\n",
    "That's right... what we have been lacking is a non-linearity.\n",
    "\n",
    "So let's build `model2` with a non-linear activation function `torch.nn.LeakyReLU`.\n",
    "\n",
    "## 6. Non-linear activation function (Exercise 5)\n",
    "\n",
    "1. Design `model2` with `torch.nn.LeakyReLU` inserted between two linear layers\n",
    "2. Train the model with `Adam` and 10000 iterations\n",
    "3. Visualize the input data and two lines from the first \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "By now, hopefully you feel familiar with a logistic regression using a linear model! and also getting used to how to define a model, optimize, and access the output using PyTorch. We will move onto a bit more complex models in the next notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
